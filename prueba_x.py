# -*- coding: utf-8 -*-
"""Prueba X.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UgItQpoMufYro-Qo_PwKEuPKEj74vUTN
"""

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.model_selection import train_test_split
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from wordcloud import WordCloud
from gensim.models import Word2Vec
import re
from textblob import TextBlob

# Funci칩n para limpiar el texto de los tweets
def clean_tweet(tweet):
    return ' '.join(re.sub(r"(@[A-Za-z0-9]+)|([^0-9A-Za-z \t])|(\w+:\/\/\S+)", " ", tweet).split())

# Funci칩n para analizar el sentimiento usando TextBlob
def get_tweet_sentiment(tweet):
    analysis = TextBlob(clean_tweet(tweet))
    if analysis.sentiment.polarity > 0:
        return 'positive'
    elif analysis.sentiment.polarity == 0:
        return 'neutral'
    else:
        return 'negative'

# Leer el archivo CSV
df = pd.read_csv('/content/Archivo/davivienda_tweets.csv')

# Limpiar los tweets
df['clean_text'] = df['Embedded_text'].apply(clean_tweet)

# Obtener los sentimientos usando TextBlob
df['textblob_sentiment'] = df['clean_text'].apply(get_tweet_sentiment)

# Crear embeddings usando Word2Vec
def create_word_embeddings(texts):
    model = Word2Vec(sentences=[text.split() for text in texts], vector_size=100, window=5, min_count=1, workers=4)
    return model

word_model = create_word_embeddings(df['clean_text'])

# Crear un diccionario de emojis con embeddings manuales o preentrenados
emoji_dict = {
    '游땕': np.random.rand(100), '游땍': np.random.rand(100), # A침adir m치s emojis seg칰n el dataset
    '游땨': np.random.rand(100), '游땴': np.random.rand(100)
}

# Combinar los embeddings de texto y emojis
def get_combined_embedding(text, emojis):
    # Crear embedding de texto
    text_embedding = np.mean([word_model.wv[word] for word in text.split() if word in word_model.wv], axis=0)

    # Verificar si el embedding de texto est치 vac칤o y si es as칤, asignar un vector de ceros
    if isinstance(text_embedding, float):  # Esto ocurre si np.mean() retorna NaN
        text_embedding = np.zeros(100)

    # Verificar si emojis es un string y no NaN
    if isinstance(emojis, str):
        emoji_embedding = np.mean([emoji_dict[emoji] for emoji in emojis if emoji in emoji_dict], axis=0)
        # Verificar si el embedding de emojis est치 vac칤o y si es as칤, asignar un vector de ceros
        if isinstance(emoji_embedding, float):  # Esto ocurre si np.mean() retorna NaN
            emoji_embedding = np.zeros(100)
    else:
        # Si no hay emojis, devolvemos un array de ceros
        emoji_embedding = np.zeros(100)

    # Combinar embeddings de texto y emojis (ambos de tama침o 100)
    return np.concatenate((text_embedding, emoji_embedding))

df['combined_embedding'] = df.apply(lambda row: get_combined_embedding(row['clean_text'], row['Emojis']), axis=1)

# Codificaci칩n de etiquetas
label_encoder = LabelEncoder()
df['label'] = label_encoder.fit_transform(df['textblob_sentiment'])

# Definir el dataset para PyTorch
class TweetDataset(Dataset):
    def __init__(self, embeddings, labels):
        self.embeddings = embeddings
        self.labels = labels

    def __len__(self):
        return len(self.embeddings)

    def __getitem__(self, idx):
        return torch.tensor(self.embeddings[idx], dtype=torch.float32), torch.tensor(self.labels[idx], dtype=torch.long)

# Crear conjuntos de entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(df['combined_embedding'].tolist(), df['label'].tolist(), test_size=0.2, random_state=42)

train_dataset = TweetDataset(X_train, y_train)
test_dataset = TweetDataset(X_test, y_test)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

# Definir el modelo con una capa de atenci칩n
class SentimentModel(nn.Module):
    def __init__(self):
        super(SentimentModel, self).__init__()
        self.attention_layer = nn.Linear(200, 1)  # La atenci칩n debe generar pesos por palabra
        self.fc = nn.Linear(200, 3)  # El tama침o de entrada es 200 y la salida tiene 3 clases

    def forward(self, x):
        # Aplicar la atenci칩n
        attention_weights = torch.softmax(self.attention_layer(x), dim=1)
        # Aplicar los pesos de atenci칩n
        x = x * attention_weights
        # Sumar los resultados ponderados (colapsar la dimensi칩n de palabras)
        x = torch.sum(x, dim=1)
        # Pasar por la capa final completamente conectada
        output = self.fc(x)
        return output

# Crear el modelo
model = SentimentModel()

# Definir la funci칩n de p칠rdida y el optimizador
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Entrenar el modelo
num_epochs = 5
for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0
    for embeddings, labels in train_loader:
        optimizer.zero_grad()
        # Aseg칰rate de que los embeddings tengan la forma correcta
        embeddings = embeddings.view(-1, 200).unsqueeze(1)  # Ajuste de la forma del tensor
        outputs = model(embeddings)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()

    print(f"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}")

# Evaluar el modelo en el conjunto de prueba
model.eval()
correct = 0
total = 0
with torch.no_grad():
    for embeddings, labels in test_loader:
        # Aseg칰rate de que los embeddings tengan la forma correcta durante la evaluaci칩n
        embeddings = embeddings.view(-1, 200).unsqueeze(1)  # Ajuste de la forma del tensor
        outputs = model(embeddings)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

accuracy = 100 * correct / total
print(f"Accuracy of the model on test data: {accuracy}%")

# An치lisis adicional y visualizaci칩n

# 1. Agrupaci칩n de Tweets por Categor칤as (Clustering)

# Vectorizaci칩n de los textos usando TF-IDF
vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')
X = vectorizer.fit_transform(df['clean_text'])

# Usamos K-Means para agrupar los tweets en 3 clusters
kmeans = KMeans(n_clusters=3, random_state=42)
df['cluster'] = kmeans.fit_predict(X)

# 2. Visualizaci칩n de Datos

# a. Distribuci칩n de Sentimientos
plt.figure(figsize=(10, 6))
sns.countplot(x='textblob_sentiment', data=df, palette='viridis')
plt.title('Distribuci칩n de Sentimientos en los Tweets')
plt.xlabel('Sentimiento')
plt.ylabel('Cantidad de Tweets')
plt.show()

# b. Nube de Palabras para cada Cl칰ster
for i in range(3):
    plt.figure(figsize=(8, 6))
    cluster_text = ' '.join(df[df['cluster'] == i]['clean_text'])
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(cluster_text)
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.title(f'Nube de Palabras para el Cl칰ster {i}')
    plt.axis('off')
    plt.show()

# c. Distribuci칩n de Clusters
plt.figure(figsize=(10, 6))
sns.countplot(x='cluster', data=df, palette='viridis')
plt.title('Distribuci칩n de Tweets por Cl칰ster')
plt.xlabel('Cl칰ster')
plt.ylabel('Cantidad de Tweets')
plt.show()

# 3. Insights y Recomendaciones

# Resumen de Insights
for i in range(3):
    print(f"Cl칰ster {i}:")
    top_terms = X[df['cluster'] == i].toarray().sum(axis=0).argsort()[-10:][::-1]
    terms = [vectorizer.get_feature_names_out()[j] for j in top_terms]
    print("T칠rminos m치s comunes:", terms)
    print()

# Recomendaciones para Marketing y Servicio al Cliente
print("Recomendaciones:")
print("- Para los tweets positivos, reforzar las campa침as de marketing en torno a las 치reas destacadas en los tweets.")
print("- Para los tweets negativos, abordar r치pidamente las quejas comunes, especialmente aquellas relacionadas con servicio al cliente.")
print("- Los clusters pueden revelar subgrupos de clientes con intereses comunes, lo que permite dise침ar campa침as de marketing espec칤ficas para esos grupos.")
print("- Utiliza los insights de los clusters para mejorar la segmentaci칩n de mercado y personalizar la comunicaci칩n seg칰n las necesidades y sentimientos espec칤ficos de cada grupo.")
print("- Analiza los t칠rminos m치s comunes en los clusters negativos para identificar 치reas de mejora en el servicio al cliente y tomar acciones correctivas.")

!pip install vaderSentiment
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

# Inicializar VADER
analyzer = SentimentIntensityAnalyzer()

# Funci칩n para obtener el sentimiento usando VADER
def vader_sentiment(tweet):
    score = analyzer.polarity_scores(tweet)
    if score['compound'] >= 0.05:
        return 'positive'
    elif score['compound'] <= -0.05:
        return 'negative'
    else:
        return 'neutral'

# Aplicar VADER para analizar los sentimientos de los tweets
df['vader_sentiment'] = df['clean_text'].apply(vader_sentiment)

# Visualizar la nueva distribuci칩n de sentimientos
plt.figure(figsize=(10, 6))
sns.countplot(x='vader_sentiment', data=df, palette='viridis')
plt.title('Distribuci칩n de Sentimientos en los Tweets usando VADER')
plt.xlabel('Sentimiento')
plt.ylabel('Cantidad de Tweets')
plt.show()

!pip install transformers
!pip install torch

from transformers import pipeline

# Cargar un pipeline preentrenado de an치lisis de sentimientos de BERT
sentiment_analysis = pipeline("sentiment-analysis")

# Aplicar BERT para analizar los sentimientos de los tweets
def bert_sentiment(tweet):
    result = sentiment_analysis(tweet)[0]
    if result['label'] == 'POSITIVE':
        return 'positive'
    elif result['label'] == 'NEGATIVE':
        return 'negative'
    else:
        return 'neutral'

df['bert_sentiment'] = df['clean_text'].apply(bert_sentiment)

# Visualizar la nueva distribuci칩n de sentimientos con BERT
plt.figure(figsize=(10, 6))
sns.countplot(x='bert_sentiment', data=df, palette='viridis')
plt.title('Distribuci칩n de Sentimientos en los Tweets usando BERT')
plt.xlabel('Sentimiento')
plt.ylabel('Cantidad de Tweets')
plt.show()

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans

# Convertir los tweets a una matriz TF-IDF
vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)
X = vectorizer.fit_transform(df['clean_text'])

# Aplicar KMeans para agrupar los tweets en clusters
kmeans = KMeans(n_clusters=5, random_state=42)  # Cambia n_clusters seg칰n tus necesidades
df['cluster'] = kmeans.fit_predict(X)

# Mostrar los t칠rminos m치s representativos de cada cluster
terms = vectorizer.get_feature_names_out()
for i in range(5):  # Ajusta el rango a n_clusters
    print(f"Cluster {i}:")
    print(", ".join([terms[ind] for ind in kmeans.cluster_centers_.argsort()[:, -1:-11:-1][i]]))

# Asumamos que ya tienes identificados los clusters y sus categor칤as
categories = {0: 'Marketing', 1: 'Servicio al Cliente', 2: 'Producto', 3: 'Competencia', 4: 'Otro'}

# Mapear las categor칤as a los clusters
df['category'] = df['cluster'].map(categories)

plt.figure(figsize=(12, 8))
sns.countplot(x='category', data=df, palette='muted')
plt.title('Distribuci칩n de Tweets por Categor칤a')
plt.xlabel('Categor칤a')
plt.ylabel('Cantidad de Tweets')
plt.show()

plt.figure(figsize=(14, 10))
sns.countplot(x='category', hue='bert_sentiment', data=df, palette='coolwarm')
plt.title('Distribuci칩n de Sentimientos por Categor칤a')
plt.xlabel('Categor칤a')
plt.ylabel('Cantidad de Tweets')
plt.show()